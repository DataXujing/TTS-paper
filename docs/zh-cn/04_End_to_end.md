## End-to-End Models

完全端到端的模型，输入文本特征直接输出音频波形如下图所示：

<div align=center>
    <img src="zh-cn/img/ch4/01/p1.png" /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch4/01/p2.png" /> 
</div>

### 1. VITS / VITS2

#### 1. VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech

!> arxiv (2021): https://arxiv.org/abs/2106.06103

!> github: https://github.com/jaywalnut310/vits

!> demo: https://jaywalnut310.github.io/vits-demo/index.html

##### Abstract

最近已经提出了几种支持单阶段训练(single-stage training)和并行采样(parallel sampling)的端到端文本转语音 （TTS） 模型，但它们的生成质量不如两阶段 TTS 系统。我们提出了一种并行的端到端 TTS方法，该方法比当前的两阶段模型产生更自然的音频。
我们的方法采用变分推理（variational inference)，并通过归一化流(normalizing flows)和对抗性训练过程(adversarial training process)进行增强，从而提高了生成建模的表达能力。我们还提出了一个随机持续时间预测器(duration predictor)，以合成来自输入文本的不同节奏(韵律rhythms)的语音。通过对潜在变量（ latent variables）和随机持续时间预测器（stochastic duration predictor）的不确定性建模，我们的方法表达了自然的一对多关系，其中一个文本输入可以以多种方式以不同的音高和节奏进行生成。对 LJ Speech（单个说话人数据集）的主观人工评估（平均意见分数，或 MOS）表明，我们的方法优于最好的公开可用的 TTS 系统，并实现了与GT相当的MOS得分。

##### 1.Introduction

文本到语音转换 （TTS） 系统通过多个组件从给定文本合成原始语音波形。随着深度神经网络的快速发展，TTS 系统管道已简化为除文本预处理（如文本规范化和音素化）之外的两阶段生成建模。第一阶段是从预处理的文本中生成中间语音表示，例如梅尔频谱图（Shen et al.， 2018）或语言特征（Oord et al.， 2016）。第二阶段是生成以中间表示为条件的原始波形（Oord et al.， 2016;Kalchbrenner等 人，2018 年）。每个两阶段管道的模型都是独立开发的。

（相关工作的总结）基于神经网络的自回归 TTS 系统已经显示出合成真实语音的能力（Shen et al.， 2018;Li et al.， 2019），但它们的顺序生成过程使得难以充分利用现代并行处理器。为了克服这一限制并提高合成速度，已经提出了几种非自回归方法。在文本到频谱图生成步骤中，从预先训练的自回归教师网络中提取注意力图（任 et al.， 2019;Peng et al.， 2020）试图降低学习文本和频谱图之间对齐的难度。最近，基于似然的方法通过估计或学习最大化目标梅尔频谱图可能性的比对，进一步消除了对外部对准器的依赖（Zeng等人 ，2020 年;Miao et al.， 2020;Kim et al.， 2020）的同时，生成对抗网络 （GAN） （Goodfellow et al.， 2014） 已在第二阶段模型中进行了探索。基于 GAN 的前馈网络具有多个判别器，每个判别器区分不同尺度或周期的样本，可实现高质量的原始波形合成（Kumar et al.， 2019;Bińkowski et al.， 2019;Kong等 人，2020 年）。

尽管并行 TTS 系统取得了进展，但两阶段管道仍然存在问题，因为它们需要顺序训练或微调（Shen et al.， 2018;Weiss et al.， 2020） 进行高质量生成语音，其中第二阶段的模型是使用第一阶段的模型生成的样本进行训练的。此外，它们对预定义中间特征的依赖性排除了学习隐藏表示来进一步提高性能的可能性。最近，FastSpeech 2s和 EATS （Donahue et al.， 2021）等几项工作提出了有效的端到端训练方法，例如对短音频剪辑而不是整个波形进行训练，利用梅尔频谱图解码器来帮助文本表示学习，并设计专门的频谱图损失来放宽目标和生成语音之间的长度不匹配的问题。然而，尽管利用学习到的表示可能会提高性能，但它们的生成的音频的综合质量落后于两阶段系统。

在这项工作中，我们提出了一种并行的端到端 TTS 方法，该方法比当前的两阶段模型产生更自然的音频。使用变分自动编码器 （VAE） （Kingma & Welling， 2014），我们通过潜在变量连接 TTS 系统的两个模块，以实现高效的端到端学习。为了提高我们方法的表现力，以便合成高质量的语音波形，我们将归一化流应用于波形域的条件先验分布和对抗训练。除了生成细粒度的音频外，TTS 系统还必须表达一对多关系，其中文本输入可以以多种方式以不同的变化（例如，音高和持续时间）说出。为了解决一对多问题，我们还提出了一个随机持续时间预测器，以合成来自输入文本的不同节奏（韵律）的语音。通过对潜在变量和随机持续时间预测器的不确定性建模，我们的方法捕获了无法用文本表示的语音变化。

与最好的公开可用的 TTS 系统相比，我们的方法获得了更自然的语音和更高的采样效率，即 Glow-TTS（Kim et al.， 2020）和 HiFi-GAN（Kong et al.， 2020）。我们公开了演示页面和源代码。


##### 2.Method

<div align=center>
    <img src="zh-cn/img/ch4/01/p3.png" /> 
</div>

在本节中，我们将解释我们提出的方法及其架构。所提出的方法主要在前三个小节中描述：条件 VAE 公式;从变分推理得出的对齐估计;提高合成质量的对抗性训练。本节末尾介绍了整体架构。图 1（a） 和 1（b） 分别显示了我们方法的训练和推理程序。从现在开始，我们将我们的方法称为端到端文本转语音 （VITS） 的对抗性学习的变分推理。

###### 2.1 Varuational Inference

**2.1.1 Overview**

VITS可以表示为条件VAE, 其目的是最大化难以处理的数据编辑多数似然 $\log p_{\theta}(x|c)$ 的变分下限,也成为证据下限( the evidence lower bound (ELBO))：

<div align=center>
    <img src="zh-cn/img/ch4/01/p4.png" /> 
</div>

其中 $p_{\theta}(z|c)$ 表示给定条件$c$的潜在变量$z$的先验分布，$p_{\theta}(x|z)$是数据点$x$的似然函数，$q_{\phi}(z|x)$是近似的后验分布。训练损失是负的ELBO,可以看做是重建损失$-\log p_{\theta}(x|z)$和KL散度$\log q_{\phi}(z|x)-\log p_{\theta}(z|c)$的和，其中$z\sim q_{\phi}(z|x)$。

**2.1.2 Reconstruction loss(重建损失)** 

作为重建损失的目标数据点，我们使用梅尔频谱图而不是原始波形，用 $x_{mel}$ 表示。我们通过解码器将潜在变量 $z$
 上采样到波形域 $\hat{y}$,并将$\hat{y}$转换为梅尔频谱域$\hat{x}_ {mel}$ 。然后， 使用$L_1$损失计算预测和目标的mel-spectrogram作为重建损失:

$$L_{recon}=||x_{mel}-\hat{x}_ {mel}||_{1}$$ 

这可以看作是假设数据分布为拉普拉斯分布并忽略常数项的最大似然估计。我们定义了梅尔频谱图域中的重建损失，通过使用近似于人类听觉系统响应的梅尔量表来提高感知质量。请注意，从原始波形进行梅尔频谱图估计不需要可训练的参数，因为它只使用 STFT 和线性投影到梅尔尺度上。此外，估计仅在训练期间使用，而不是推理。在实践中，我们不去上采样整个潜在变量$z$,而是使用部分序列作为解码器的输入，窗口化生成器用于高效的端到端的训练 (Ren et al., 2021; Donahue et al., 2021)。


**2.1.3 KL散度**

先验编码器（prior encoder) $c$ 的输入条件由从文本中提取的音素（phonemes)$c_{text}$和音素与潜在变量之间的对齐$A$组成。
对齐是一个硬单调的注意力矩阵，其$|c_{text}| \times |z|$维度表示每个输入音素扩展为与目标语音进行时间对齐的时间。由于对齐没有GT标签，因此我们必须在每次训练迭代中估计对齐，我们将在2.2.1节中讨论。在我们的问题设置中，我们的目标是为后验编码器（posterior encoder)通过平更高分辨率的信息。因此，我们使用目标语音的线性尺度频谱图$x_{lin}$作为输入，而不是梅尔频谱图。请注意，修改后的输入并不违反变分推理的特性。KL散度的定义为：

<div align=center>
    <img src="zh-cn/img/ch4/01/p5.png" /> 
</div>

因式分解的正态分布用于参数化我们的先验和后验编码器。我们发现，提高先验分布的表达性对于生成真实样本很重要。因此，我们应用了一个归一化流 $f_{\theta}$
 （Rezende & Mohamed， 2015），它允许在因子化的正态先验分布之上，按照变量变化的规则，将简单分布逆向转换为更复杂的分布：

<div align=center>
    <img src="zh-cn/img/ch4/01/p6.png" /> 
</div>


###### 2.2 Alignment Estimation

**2.2.1 Monotonic alignment search（单调对齐搜索）**

为了估计输入文本和目标语音之间的对齐 $A$方式，我们采用了单调对齐搜索 （MAS） （Kim et al.， 2020），这是一种搜索对齐方式的方法，该方法使数据通过normalizing flow $f$ 参数化的可能性最大化：

<div align=center>
    <img src="zh-cn/img/ch4/01/p7.png" /> 
</div>

其中候选对齐被限制为单调和非跳跃，因为人类按顺序阅读文本而不会跳过任何单词。 为了找到最佳对齐方式，Kim et al. （2020） 使用了动态规划。在我们的设置中直接应用 MAS 很困难，因为我们的目标是 ELBO，而不是确切的对数似然。因此，我们重新定义 MAS 以找到使 ELBO 最大化，这归结为找到使潜在变量 $z$ 的对数似然最大化：

<div align=center>
    <img src="zh-cn/img/ch4/01/p8.png" /> 
</div>

由于等式5与等式6相似，我们可以在不修改的情况下使用原始 MAS 实现。附录 A 包括 MAS 的伪代码。

**2.2.2 Duration prediction from text**

我们可以通过对 $\sum_{j} A_{i,j}$的每一行中的所有列求和来计算每个输入token $d_i$的持续时间。正如之前的工作（Kim et al.， 2020）所提出的，持续时间可用于训练确定性的持续时间预测器，但它无法表达一个人每次都以不同的语速说话的方式。为了生成类似人类的语音节奏，我们设计了一个随机持续时间预测器，以便其样本遵循给定音素的持续时间分布。随机持续时间预测器是一种基于流的生成模型，通常通过最大似然估计进行训练。然而，直接应用最大似然估计是困难的，因为每个输入音素的持续时间是 

1） 一个离散整数，需要对其进行反量化才能使用连续归一化流，

2） 一个标量，它可以防止由于可逆性而导致的高维变换。

我们应用变分反量化（Ho et al.， 2019）和变分数据增强（Chen et al.， 2020）来解决这些问题。具体来说，我们引入了两个随机变量 $u$和 $ν$，它们与持续时间序列 $d$的时间分辨率和维度 相同，分别用于变分反量化和变分数据增强。我们将$u$的值限制为$[0,1)$，以便$d-u$的差值变成正实数序列，并且我们沿channel concat $ν$和 $d$以生成更高维度的潜在表示。我们通过近似的后验分布 
$q_{\phi}(u,v|d,c_{text})$对两个变量进行采样。结果目标是音素持续时间的对数似然的变分下限：

<div align=center>
    <img src="zh-cn/img/ch4/01/p9.png" /> 
</div>

然后，训练损失 $L_{dur}$是负变分下限。我们将停止梯度运算符（van den Oord et al.， 2017）应用于输入条件，以防止输入梯度的反向传播，以便持续时间预测器的训练不会影响其他模块的训练。

采样过程相对简单;音素持续时间是通过随机持续时间预测器的逆变换从随机噪声中采样的，然后将其转换为整数。

###### 2.3 Adversarial Training

为了在我们的学习系统中采用对抗性训练，我们添加了一个判别器 D
 ，用于区分解码器 G生成的输出和真值波形y
 。在这项工作中，我们使用了两种成功应用于语音合成的损失;用于对抗性训练的最小二乘损失函数（Mao et al., 2017) 和用于训练生成器的附加特征匹配损失（Larsen et al.， 2016）：

<div align=center>
    <img src="zh-cn/img/ch4/01/p10.png" /> 
</div>

其中$T$表示判别器的总层数，$D^l$输出判别器第$l$层的特征，$N_l$是第$l$层特征的特征数。值得注意的是，特征匹配损失可以看做是在判别器的隐藏层中测量的重建损失，建议作为VAE元素重建损失的替代方案(Larsen et al., 2016).

###### 2.4 Final Loss

将 VAE 和 GAN 训练相结合，训练我们的条件 VAE 的总损失可以表示如下：

<div align=center>
    <img src="zh-cn/img/ch4/01/p11.png" /> 
</div>

###### 2.5 Model Architecture

所提出的模型的整体架构由后验编码器、先验编码器、解码器、判别器和随机持续时间预测器组成。后验编码器和判别器仅用于训练，不用于推理。附录 B 中提供了体系结构详细信息。

**2.5.1 Posterior Encoder**

对于Posterior Encoder，我们使用 WaveGlow（Prenger et al.， 2019）和 Glow-TTS（Kim et al.， 2020）中使用的非因果 WaveNet 残差块。WaveNet 残差块由具有gated activation unit 和skip connection的膨胀卷积层组成。拼接线性投影层生成正态后验分布的均值和方差。对于多说话人的情况，我们在残差块中使用全局条件 （Oord et al.， 2016） 来添加说话人嵌入(speaker embedding)。

**2.5.2 Prior Encoder**

先验编码器由处理输入音素$c_{text}$的文本编码器 和提高先验分布灵活性的规范化流 $f_{theta}$组成。文本编码器是一种 transformer 编码器（Vaswani et al.， 2017），它使用相对位置表示（Shaw et al.， 2018）而不是绝对位置编码。$c_{text}$ 可以通过文本编码器和文本编码器上方的线性投影层获得隐藏的表示 $h_{text}$，该层产生用于构建先验分布的平均值和方差。归一化流是一堆仿射耦合层（Dinh et al.， 2017），由一堆 WaveNet 残差块组成。为简单起见，我们将规范化流设计为雅可比行列式为 1 的体积保留转换。对于 multi-speaker 设置，我们将 speaker embedding 添加到通过全局调节的归一化流中的残差块中。

**2.5.3 Decoder**

解码器本质上是 HiFi-GAN V1 生成器（Kong et al.， 2020）。它由一堆转置卷积组成，每个卷积后面都有一个多感受野融合模块 （MRF）。MRF 的输出是具有不同感受野大小的残差块的输出之和。对于多扬声器设置，我们添加一个线性层来转换扬声器嵌入，并将其添加到 input latent 变量 $z$中。

**2.5.4 Discriminator（判别器）**

我们遵循 HiFi-GAN 中提出的多周期判别器的判别器架构 （Kong et al.， 2020）。多周期判别器是马尔可夫基于窗口的子判别器（Kumar et al.， 2019）的混合体，每个子判别器都对输入波形的不同周期模式进行操作。

**2.5.5 Stochastic Duration Predictor**

随机持续时间预测器根据条件输入 $h_{text}$估计音素持续时间的分布。为了对随机持续时间预测器进行有效的参数化，我们将残差块与膨胀和深度可分离的卷积层堆叠在一起。我们还将神经样条流（Durkan et al.， 2019）应用于耦合层，它通过使用单调有理二次样条以可逆非线性变换的形式出现。与常用的仿射耦合层相比，神经样条流通过相似数量的参数提高了转换表现力。对于多扬声器设置，我们添加一个线性层来转换扬声器嵌入并将其添加到 input $h_{text}$中。

##### 3.Experiments

###### 3.1 Datasets

我们在两个不同的数据集上进行了实验。我们使用 LJ Speech数据集 （Ito， 2017） 与其他公开可用的模型进行比较和 VCTK 数据集 （Veaux et al.， 2017）用来验证我们的模型是否可以学习和表达不同的语音特征。LJ Speech 数据集由单个说话人的 13100 个简短音频剪辑组成，总时长约为 24 小时。音频格式是 16 位 PCM，采样率为 22 kHz，我们无需任何操作即可使用它。我们将数据集随机分为训练集（12500 个样本）、验证集（100 个样本）和测试集（500 个样本）。VCTK 数据集由大约 44,000 个简短的音频剪辑组成，这些剪辑由 109 名英语母语人士以各种口音说出。音频剪辑的总长度约为 44 小时。音频格式为 16 位 PCM，采样率为 44 kHz。我们将采样率降低到 22 kHz。我们将数据集随机分为训练集（43470 个样本）、验证集（100 个样本）和测试集（500 个样本）。


###### 3.2 Preprocessing

我们使用线性频谱图，该频谱图可以通过短时傅里叶变换 （STFT） 从原始波形中获得，作为后验编码器（posterior encoder）的输入。转换的 FFT 大小（FFT size）、窗口大小（window size ）和跃点大小（hop size of the transform）分别设置为 1024、1024 和 256。我们使用 80 个波段的 梅尔 尺度频谱图进行重建损失，这是通过将 梅尔滤波器组应用于线性频谱图获得的。

我们使用国际音标 （ International Phonetic Alphabet：IPA） 序列作为 prior encoder的输入。我们使用开源软件将文本序列转换为 IPA 音素序列 （Bernard， 2021），并且在 Glow-TTS 实施后，转换后的序列穿插着一个空白标记。

###### 3.3 Training

网络使用AdamW优化器（Loshchilov & Hutter，2019）进行训练，其中有 $\beta_1=0.8$, $\beta_2=0.99$
权重衰减 $\lambda=0.01$。学习率衰减为每个 epoch 因子为 $0.999^{\frac{1}{8}}$，初始学习率为 $2\times 10^{-4}$
。继以前的工作（Ren et al.， 2021;Donahue et al.， 2021）中，我们采用了窗口化生成器训练，这是一种只生成部分原始波形的方法，以减少训练期间的训练时间和内存使用。我们随机提取窗口大小为 32 的潜在表示片段馈送到解码器，而不是馈送整个潜在表示，并从GT原始波形中提取相应的音频片段作为训练目标。我们在 4 个 NVIDIA V100 GPU 上使用混合精度训练。每个 GPU 的批量大小设置为 64，模型训练多达 800k 步。


###### 3.4 Experimental Setup for Comparison

将我们的模型与最好的公开可用模型进行了比较。自回归模型 Tacotron 2 和基于流的非自回归模型 Glow-TTS 作为第一阶段模型，使用 HiFi-GAN 作为第二阶段模型。我们使用了他们的公共实现和预训练权重。由于两阶段 TTS 系统理论上可以通过顺序训练实现更高的合成质量，因此我们将微调的 HiFi-GAN 高达 100k 步长作为第一阶段模型的预测输出的声码器用来合成语音波形。我们凭经验发现，在teacher-forcing 模式下，使用 Tacotron 2 生成的梅尔频谱图对 HiFi-GAN 进行微调，导致 Tacotron 2 和 Glow-TTS 的质量比使用 Glow-TTS 生成的梅尔频谱图进行微调更好，因此我们将更好的微调 HiFi-GAN 作为 Tacotron 2 和 Glow-TTS的声码器。

由于每个模型在采样过程中都有一定程度的随机性，因此我们在整个实验过程中修复了控制每个模型随机性的超参数。Tactron 2 的 pre-net 中 dropout 的概率设置为 0.5。对于 Glow-TTS，先验分布的标准差设置为 0.333。对于 VITS，随机持续时间预测器的输入噪声的标准差设置为 0.8，我们将比例因子 0.667 乘以先验分布的标准差。


##### 4.Results

###### 4.1 Speech Synthesis Quality

我们进行了众包的 MOS 测试来评估质量。评分者聆听随机选择的音频样本，并按照从 1 到 5 的 5 分制对它们的自然度进行评分。允许评分者对每个音频样本评估一次，并且我们对所有音频剪辑进行了标准化，以避免振幅差异对分数的影响。这项工作中的所有质量评估都是以这种方式进行的。

评估结果如表 1 所示。VITS 的性能优于其他 TTS 系统，并实现了与GT相似的 MOS。VITS （DDP） 采用与 Glow-TTS 相同的确定性持续时间预测器架构，而不是随机持续时间预测器，在 MOS 评估中得分在 TTS 系统中排名第二。这些结果表明：

1） 随机持续时间预测器比确定性持续时间预测器生成更真实的音素持续时间

2） 我们的端到端训练方法是生成比其他 TTS 模型更好的样本的有效方法，即使保持相似的持续时间预测器架构。

<div align=center>
    <img src="zh-cn/img/ch4/01/p12.png" /> 
</div>

我们进行了一项消融研究以证明我们方法的有效性，包括先验编码器中的归一化流和线性标度频谱图后验输入。消融研究中的所有模型都经过了高达 300k 步的训练。结果如表 2 所示。去除先验编码器中的归一化流会导致比基线降低 1.52 MOS，这表明先验分布的灵活性会显著影响综合质量。用梅尔频谱图替换后验输入的线性尺度频谱图会导致质量下降 （-0.19 MOS），表明高分辨率信息对 VITS 在提高合成质量方面是有效的。

<div align=center>
    <img src="zh-cn/img/ch4/01/p13.png" /> 
</div>

###### 4.2 Generalization to Multi-Speaker Text-to-Speech

为了验证我们的模型可以学习和表达不同的语音特征，我们将我们的模型与 Tacotron 2、Glow-TTS 和 HiFi-GAN 进行了比较，它们显示了扩展到多说话人语音合成的能力（Jia et al.， 2018;Kim et al.， 2020;Kong等 人，2020 年）。我们在 VCTK 数据集上训练了模型。我们按照 2.5 节 中的描述为模型添加了扬声器嵌入。对于 Tacotron 2，我们广播了扬声器嵌入并将其与编码器输出连接，对于 Glow-TTS，我们按照之前的工作应用了全局调节。评估方法与 Section 4.1 中描述的方法相同。如表 3 所示，我们的模型实现了比其他模型更高的 MOS。这表明我们的模型以端到端的方式学习和表达各种语音特征

<div align=center>
    <img src="zh-cn/img/ch4/01/p14.png" /> 
</div>

###### 4.3 Speech Variation

我们验证了随机持续时间预测器产生多少种不同的语音长度，以及合成样本有多少种不同的语音特征。与 Valle et al. （2021） 类似，这里的所有样本都是从一句话“有多少变化？图 2（a） 显示了每个模型生成的 100 个话语的长度直方图。虽然由于确定性持续时间预测器，Glow-TTS 仅生成固定长度的话语，但我们模型中的样本遵循与 Tacotron 2 相似的长度分布。图 2（b） 显示了在多说话人设置中，我们的模型中的 5 个说话人身份中的每一个生成的 100 个话语的长度，这意味着该模型学习了与说话人相关的音素持续时间。在图3中，用YIN算法提取的10个话语的F0轮廓（De Cheveigné & Kawahara，2002）显示我们的模型生成具有不同音调和节奏的语音，而图3（d）中每个不同说话人身份生成的五个样本表明，我们的模型对每个说话人身份表达的语音长度和音高非常不同。请注意，Glow-TTS 可以通过增加先验分布的标准差来增加音高的多样性，但相反，它可能会降低合成质量。

<div align=center>
    <img src="zh-cn/img/ch4/01/p15.png" /> 
</div>

###### 4.4 Synthesis Speed

我们将模型的合成速度与并行的两阶段 TTS 系统 Glow-TTS 和 HiFi-GAN 进行了比较。我们测量了整个过程中的同步运行时间，以从 LJ Speech 数据集的测试集中随机选择 100 个句子的音素序列生成原始波形。我们使用了单个 NVIDIA V100 GPU，批量大小为 1。结果如表 4 所示。由于我们的模型不需要模块来生成预定义的中间表示，因此它的采样效率和速度大大提高。


<div align=center>
    <img src="zh-cn/img/ch4/01/p16.png" /> 
</div>


##### 5.Related Work

###### 5.1 End-to-End Text-to-Speech

目前，两阶段的神经 TTS 模型可以合成类似人类的语音（Oord et al.， 2016;Ping et al.， 2018;Shen et al.， 2018）。但是，它们通常需要使用第一阶段模型输出进行训练或微调声码器，这会导致训练和部署效率低下。他们也无法获得端到端方法的潜在好处，该方法可以使用学习的隐藏表示而不是预定义的中间特征。

最近，有人提出了单阶段的端到端 TTS 模型，以解决直接从文本生成原始波形的更具挑战性的任务，这些波形包含比梅尔频谱图更丰富的信息（例如，高频响应和相位）。FastSpeech 2s（Ren et al.， 2021）是 FastSpeech 2 的扩展，它通过采用对抗训练和辅助梅尔频谱图解码器来帮助学习文本表示，从而实现端到端并行生成。但是，为了解决一对多问题，FastSpeech 2s 必须从用作训练输入条件的语音中提取音素持续时间、音高和能量。EATS （Donahue et al.， 2021） 也采用了对抗性训练和可微分的对齐方案。为了解决生成语音和目标语音之间的长度不匹配问题，EATS 采用了动态规划计算的软动态时间扭曲损失。Wave Tacotron （Weiss et al.， 2020） 将归一化流与 Tacotron 2 相结合，以实现端到端结构，但仍然是自回归的。上述所有端到端 TTS 模型的音频质量都低于两阶段模型。

与上述端到端模型不同，通过使用条件 VAE，我们的模型 

1） 学习直接从文本合成原始波形，而无需额外的输入条件

2） 使用动态规划方法 MAS 来搜索最佳对齐而不是计算损耗

3） 并行生成样本

4） 优于最好的公开两阶段模型。


###### 5.2 Variational Autoencoders

VAEs（Kingma & Welling，2014）是使用最广泛的基于可能性的深度生成模型之一。我们对 TTS 系统采用有条件的 VAE。条件 VAE 是一种条件生成模型，其中观察到的条件调节用于生成输出的潜在变量的先验分布。在语音合成中，Hsu et al. （2019） 和 Zhang et al. （2019） 将 Tacotron 2 和 VAEs 结合起来学习语音风格和韵律。BVAE-TTS（Lee等 人，2021 年）基于双向 VAE 并行生成梅尔频谱图（Kingma等人 ，2016 年）。与之前将 VAE 应用于第一阶段模型的工作不同，我们将 VAE 应用于并行的端到端 TTS 系统。

Rezende & Mohamed （2015），Chen et al. （2017） 和Ziegler & Rush （2019）通过增强前验和后验分布的表现力和标准化流来提高VAE性能。为了提高先验分布的表示能力，我们在条件先验网络中添加了归一化流，从而生成了更真实的样本。

与我们的工作类似，Ma et al. （2019） 提出了一种条件 VAE，在非自回归神经机器翻译的条件先验网络中对流进行归一化，即 FlowSeq。然而，我们的模型可以显式地将潜在序列与源序列进行比对这一事实与 FlowSeq 不同，后者需要通过注意力机制来学习隐式比对。我们的模型通过 MAS 将潜在序列与时间对齐的源序列进行匹配，从而消除了将潜在序列转换为标准正态随机变量的负担，从而简化了归一化流的架构。

###### 5.3 Duration Prediction in Non-Autoregressive Text-to-Speech

自回归 TTS 模型（Taigman等 人，2018 年;Shen et al.， 2018;Valle et al.， 2021）通过其自回归结构和几种技巧（包括在推理和启动过程中保持辍学概率）生成具有不同节奏的多样化语音（Graves，2013）。 并行 TTS 模型（Ren et al.， 2019;Peng et al.， 2020;Kim et al.， 2020;Ren et al.， 2021;Lee et al.， 2021）一直依赖于确定性持续时间预测。这是因为并行模型必须预测目标音素持续时间或一条前馈路径中目标语音的总长度，这使得很难捕捉语音节奏的相关联合分布。在这项工作中，我们提出了一个基于流的随机持续时间预测器，它学习估计音素持续时间的联合分布，从而并行产生不同的语音节奏。


##### 6.Conclusion

在这项工作中，我们提出了一个并行的 TTS 系统 VITS，它可以以端到端的方式学习和生成。我们进一步引入了随机持续时间预测器来表达不同的语音节奏。生成的系统直接从文本合成自然发音的语音波形，而无需经过预定义的中间语音表示。我们的实验结果表明，我们的方法优于两阶段 TTS 系统，并达到接近人类的质量。我们希望所提出的方法将用于许多使用两阶段 TTS 系统的语音合成任务中，以实现性能改进并享受简化的训练程序。我们还想指出，尽管我们的方法在 TTS 系统中集成了两个独立的生成管道，但仍然存在文本预处理问题。研究语言表示的自我监督学习可能是删除文本预处理步骤的一个可能方向。我们将发布我们的源代码和预训练模型，以促进未来许多方向的研究。


##### A. Monotonic Alignment Search

我们在图 4 中展示了 MAS 的伪代码。尽管我们搜索的是使 ELBO 最大化，而不是数据的精确对数似然，但我们可以使用 Glow-TTS 的 MAS 实现，如第 2.2.1 节所述。

<div align=center>
    <img src="zh-cn/img/ch4/01/p17.png" /> 
</div>

##### B. Model Configurations

在本节中，我们主要描述了 VITS 的新添加部分，因为我们对模型的几个部分遵循了 Glow-TTS 和 HiFi-GAN 的配置：我们使用与 Glow-TTS 相同的Transformer编码器和 WaveNet 残差块;我们的解码器和多周期判别器分别与 HiFi-GAN 的生成器和多周期判别器相同，只是我们对解码器使用不同的输入维度，并附加了一个子判别器。

###### B.1. Prior Encoder and Posterior Encoder

先验编码器中的归一化流是四个仿射耦合层的堆栈，每个耦合层由四个 WaveNet 残差块组成。由于我们将仿射耦合层限制为体积保留变换，因此耦合层不会产生缩放参数。

后验编码器由 16 个 WaveNet 残差块组成，采用线性尺度对数幅度频谱图，并生成具有 192 个通道的潜在变量。

###### B.2. Decoder and Discriminator

我们的解码器的输入是从先验或后验编码器生成的潜在变量，因此解码器的输入通道大小为 192。对于解码器的最后一个卷积层，我们删除了一个 bias 参数，因为它会导致在混合精度训练期间出现不稳定的梯度尺度。

对于判别器，HiFi-GAN 使用包含五个周期（period)的$[2,3,5,7,11]$
 的子判别器的多周期判别器和包含三个子判别器的多尺度判别器。为了提高训练效率，我们只保留对原始波形进行操作的多尺度判别器的第一个子判别器，并丢弃对平均池化波形进行操作的两个子判别器。结果判别器可以看作是带有周期 
$[1,2,3,5,7,11]$的多周期判别器。


###### B.3. Stochastic Duration Predictor

图 5（a） 和 5（b） 分别显示了随机持续时间预测器的训练和推理过程。随机持续时间预测器的主要构建块是膨胀和深度可分离卷积 （DDSConv） 残差块，如图 5（c） 所示。DDSConv 块中的每个卷积层后跟一个层归一化层和 GELU 激活函数。我们选择使用扩张和深度可分离的卷积层来提高参数效率，同时保持较大的感受野大小。

<div align=center>
    <img src="zh-cn/img/ch4/01/p18.png" /> 
</div>

持续时间预测器中的后验编码器和归一化流模块是基于流的神经网络，具有相似的架构。区别在于，后验编码器将高斯噪声序列转换为两个随机变量$ν$,$u$表示近似后验分布 $q_{\phi}(u,v|d,c_{text})$，而归一化流模块将 $d-u$和$v$转换为高斯噪声序列，以表示增强和反量化数据的对数似然： $\log p_{theta}(d-u,v|v_{text})$
如 Section 2.2.2 中所述。

所有输入条件都通过条件编码器处理，每个条件编码器由两个 1x1 卷积层和一个 DDSConv 残差块组成。后验编码器（posterior encoder）和归一化流模块具有四个神经样条流耦合层。每个耦合层首先通过 DDSConv 模块处理输入和输入条件，并生成 29 个通道参数，用于构造 10 个有理二次函数。我们将所有耦合层和条件编码器的隐藏维度设置为 192。图 6（a） 和 6（b） 显示了随机持续时间预测器中使用的条件编码器和耦合层的架构。

<div align=center>
    <img src="zh-cn/img/ch4/01/p19.png" /> 
</div>

###### C. Side-by-Side Evaluation

我们通过对 50 个项目的 500 个评分，在 VITS 和GT之间进行了 7 分比较平均意见评分 （CMOS） 评估。我们的模型在 LJ Speech 和 VCTK 数据集上分别实现了 -0.106 和 -0.270 CMOS，如表 5 所示。这表明，尽管我们的模型优于最好的公开可用的 TTS 系统 Glow-TTS 和 HiFi-GAN，并且在 MOS 评估中取得了与GT相当的分数，但与我们的模型相比，评分者仍然偏爱GT。

<div align=center>
    <img src="zh-cn/img/ch4/01/p20.png" /> 
</div>

###### D. Voice Conversion

在多说话人设置中，我们不会在文本编码器中提供说话人身份，这使得从文本编码器估计的潜在变量学习是与说话人无关的表示。使用与说话人无关的表示形式，我们可以将一个说话人的录音转换为另一个说话人的声音。对于给定的说话人身份 
$s$和说话人的话语，我们可以从相应的话语音频中获得线性频谱图 $x_{lin}$ 。我们可以通过后验编码器和前验编码器中的归一化流将$x_{lin}$转换为 独立于说话人的表示$e$：

<div align=center>
    <img src="zh-cn/img/ch4/01/p21.png" /> 
</div>

然后，我们可以通过归一化流的逆变换$f^{-1}_ {\theta}$和解码器 $G$，从表示 $e$
中合成目标说话人$\hat{s}$ 的声音$\hat{y}$：

$$\hat{y}=G(f^{-1}_ {\theta}(e|\hat{s})|\hat{s})$$

学习独立于说话人的表示并将其用于语音转换可以看作是 Glow-TTS 中提出的语音转换方法的扩展。我们的语音转换方法提供原始波形，而不是 Glow-TTS 中的梅尔频谱图。语音转换结果如图 7 所示。它显示了具有不同音高级别的音高轨道的类似趋势。

<div align=center>
    <img src="zh-cn/img/ch4/01/p22.png" /> 
</div>

------

#### VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with
Adversarial Learning and Architecture Design

!> arxiv(2023): https://arxiv.org/abs/2307.16430

!> github: https://github.com/daniilrobnikov/vits2

!> demo: https://vits-2.github.io/demo/







------

### 2. Char2Wav


### 3. ClariNet


### 4. FastSpeech 2s

参考:[FastSpeech V2](zh-cn/03_Text_to_spectrogram?id=_9-fastspeech-2-fast-and-high-quality-end-to-end-text-to-speech)

### 5. EATS


### 6. Wave-Tacotron


### 7. JETS

!> https://arxiv.org/pdf/2203.16852.pdf


