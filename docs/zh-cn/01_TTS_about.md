## 语音合成综述
------


<!-- <div align=center>
    <img src="zh-cn/img/word2vec/p8.png" /> 
</div> -->

<!-- ### 1. Class 1: OverView -->

<!-- https://www.bilibili.com/video/BV1no4y1U7yU/?spm_id_from=333.999.0.0&vd_source=def8c63d9c5f9bf987870bf827bfcb3d -->

<!-- https://www.bilibili.com/video/BV1hZ4y1w7j1/?spm_id_from=333.337.search-card.all.click&vd_source=def8c63d9c5f9bf987870bf827bfcb3d -->

语音合成是指由文字生成语音的过程，通俗的讲就是让机器按照人的指令发出声音。早在1976年，匈牙利发明家Wolfgang von Kempelen就设计了一台
会说话的机器，如下图所示：

<div align=center>
    <img src="zh-cn/img/ch1/p1_Kempelens-speaking-machine.png" /> 
</div>
<p align=center>Saarland University大学于2007-2009年重现的Kempelen发声器</p>

这台机器用机械装置模拟人的发音机理，通过风箱驱动簧片产生声音。1845年奥地利发明家Joseph Faber发明了Euphonia，可以通过键盘发出声音。
这些早期的发声机器用机械装置模拟人的发声过程，清晰度较低，只能发一些简单的音素和单词。

计算机发明以后，语音合成技术开始快速发展。按照时间顺序，语音合成方法可以归纳为四种：**参数合成**、**拼接合成**、**统计模型合成**和
**神经模型合成**。绝大多数合成都基于激励-响应(Source-Filter)模型，我们首先对该模型做简单介绍。


### 1.激励-响应模型

人类的发音可以描述为一个声门激励序列经过一个声道响应函数的滤波的过程，称为激励-响应（Source-Filter)模型。基于这一模型，声音是肺部气流
冲击声门（主要是声带）产生的振动通过声道（包括口腔和鼻腔）共鸣产生的短时平稳周期信号。如果声带完全打开，声门产生的是随机噪声，这时产生的
声音是清辅音；如果声带拉紧，声门产生的是周期性振动，这时产生的声音是浊辅音或元音。在这一模型中，气流冲击声带被称为声门激励，声道的共鸣被称为
声道响应，也可认为是对声门激励激励信号的滤波或调制。语音生成的过程如下图所示，气流冲击声门产生混乱的或周期性的振动，这一振动通过口鼻形成的声道形成共鸣，从唇齿间发射出来形成语音。

<div align=center>
    <img src="zh-cn/img/ch1/p2_激励-声道响应模型(已去底).png"  width=70%/> 
</div>
<p align=center>人类发音的声门激励-声道响应模型</p>

我们以元音为例观察上述激励-响应模型产生的语音信号的特点。基于该模型，语音信号是声门发出的基础周期信号经过声道的共鸣。这一过程有两个特点：一方面，原始周期信号在声道所形成的共振腔中反射叠加，会生成一系列的倍频信号，在频域上表现为周期性波动；另一方面，声道具有调制作用，在不同倍频信号上
会产生不同的增益，形成特殊的频谱包络，如下图所示：

<div align=center>
    <img src="zh-cn/img/ch1/p3_频谱包络.png"  width=70%/> 
</div>

上图给出了语音信号的频谱特性，其中$F_{0}$是周期信号的频率，称为基频($F_0$),$F_1,F_2,F_3$分别为频谱包络的极大值点，称为第一，第二，第三共振峰。

需要强调的是，$F_0$表征的是声门的激励特性，而各个共振峰描述的是声道的响应函数，两者具有完全不同的意义。语音信号是倍频后的声门激励和声道响应函数的卷积，在
频域上表现为声道特性对不同频率幅值的调制，形成特别的频谱包络。上图中黑色虚线表示实际频谱，黑色实线表示频谱包络，对应声道响应函数。

激励-响应模型是一个生成模型，给定声门激励和声道响应函数，将两者卷积即可合成语音。此外，基于该模型，也可以实线语音信号的分解，将语音分解为
声门激励和声道响应两个相对独立的成分。这一分解事实上是一个解卷积的过程。最著名的分解方法是基于线性预测（LP）的的分解，该方法假设语音信号具有线性可预测性，线性预测系数（LPC）构成了声道特性，预测的残差对应声门激励。如下图所示，给出了这一分解的示意图：

<div align=center>
    <img src="zh-cn/img/ch1/p4_LP语音信号分解(已去底).png"  width=70%/> 
</div>
<p align=center>基于线性回归模型（LP）的语音信号分解示意图</p>

左图为输入语音信号；中图上方为线性预测模型的传递函数，下方为该模型对应的冲激响应；右图为预测的残差。在中图上方的线性预测模型中，$\\{a_k\\}$为线性预测系数（LPC）。

综上所述，基于激励-响应模型，我们既可以对语音进行分解，也可以基于分解得到的成分对声音进行合成。对声音进行分解-合成的装置称为声码器，其中分解部分被称为编码器，合成部分被称为解码器。历史上第一个声码器由贝尔实验室于1930年发明，这一发明奠定了语音合成的基础，下图给出了一个基于LPC的声码器的编-解码流程，其中编码器将声音分解成基频和频谱包络两个部分，这两个部分信号经过通信信道传送到接收端，解码器利用这两路信息合成语音。

<div align=center>
    <img src="zh-cn/img/ch1/p5_LPC声码器的编解码流程.png"  width=70%/> 
</div>
<p align=center>LPC声码器的编-解码流程</p>


### 2.参数合成

由上述激励-响应模型可知，只要设计合适的激励信号和合适的声道响应函数，即可合成目标语音。对这些信号进行参数化，通过选择不同的参数实现不同的发音，这是参数合成的基本思路。

前面提到，声门激励与发音的类型有关，如辅音和元音，而声道特性直接决定发音的内容。如下图给出了不同元音在共振峰平面($F_1-F_2$)上的分布情况，可以看到不同元音在这一平面上有显著区分，意味着我们只要指定$F_1$和$F_2$的值，即可生成对应的元音。在实际的系统中，可以对现有发音库进行分析，统计不同音素（包括其上下文）的基频和各个共振峰的取值，在合成时基于这些取值即可合成出相应的声音。这一方法称为参数合成。

<div align=center>
    <img src="zh-cn/img/ch1/p6_不同元音在共振峰平面上的分布.png"  width=50%/> 
</div>
<p align=center>不同元音在共振峰平面（$F_1-F_2$)上的分布情况</p>

DEC公司推出的DECTalk:<https://github.com/dectalk/dectalk>是这种合成方法的典型代表，其开发团队如下图所示：

<div align=center>
    <img src="zh-cn/img/ch1/p7_dectalk.png"  width=50%/> 
</div>
<p align=center>DECTalk开发团队</p>

参数合成的优点是计算量小、可调节性高，缺点是参数调节困难，生成的声音自然度较低。


### 3.拼接合成

20世纪90年代，随着大规模语音库的积累，基于拼接的合成方法成为主流。这一方法将事先录好的语音切分成发音片段（一般为音素），在合成时从这些片段中
选择合适的候选片段进行拼接，组装成句子。

拼接方法需要处理2个主要问题。第一个问题是对上下文环境的处理，同一个音素在不同环境下发音会有所差别，因此需要根据环境选择合适的发音片段。这里的环境既包括前后音素，也可以包括词边界，词性，句子属性等。第二个问题是拼接连续性问题。把两个发音片段拼在一起时，总会产生一个不连续性。一般采用在频域进行拼接和平滑的方法来增加连续性。

早期的拼接合并多采用半音素(Di-Phone)作为发音片段。所谓半音素，是从一个音素的中心到下一个音素的中心对应的发音片段，如下图所示，B-IY,IY-R等都是半音素。半音素模型有三个好处，一是在进行语音标注时比较方便，因为标注音素的中心比标注音素的边界容易的多；二是半音素中包含了前后音素的转换过程，有利于对上下文关系的描述；三是音素中心一般都比较稳定，拼接时有利于保持发音的连续性。

<div align=center>
    <img src="zh-cn/img/ch1/p8_语音流中的半音素.png"  width=50%/> 
</div>
<p align=center>语音流中的半音素</p>

另一种语音拼接合成方法是基于单元选择（Unit Selection）的方法，如下图所示，在这种方法里，每个发音片段是一个上下文相关的音素，称为1个单元。不同的音素可能有多个单元，每个单元具有环境相关的各种标记，以及相应的基频，频谱包络等信息。在合成时，按目标句子的上下文环境选择合适的单元，将相应的基频和频谱包络拼接起来，既可以合成一个完整的句子。在单元选择的过程中，需要考虑两个准则，一是选择出的单元应符合环境约束，二是选择出的单元在互相拼接时应保持连贯性。和半音素方法相比，单元选择方法基于更大规模的数据库，对环境的建模更细致。理论上说，如果数据库规模足够大，我们总可以选择出合适的单元，生成自然流畅的语音。

<div align=center>
    <img src="zh-cn/img/ch1/p9_语音拼接合成.png"  width=50%/> 
</div>
<p align=center>基于单元选择的语音拼接合成</p>


### 4.统计模型合成

基于拼接的合成方法可以生成高质量的语音，但需要较大规模的语音库，录制成本较高，占用磁盘空间大，在嵌入式设备上很难使用。另外，拼接方法无法灵活的改变声音的特性，在应用上有一定的局限性。

为解决拼接方法的这些困难，人们提出统计模型方法。该方法为每个发音单元建立一个统计模型，在合成时仅利用这些模型生成语音，而不需要原始的语音库，因此该系统通常比较精简。特别重要的是，基于统计模型可以很容易的实现语音特性的控制，这对个性化合成尤为重要。基于HMM的方法是这一时期的主流。这种方法对每个发音单元建立一个HMM模型，在合成时将句子中所有发音单元的HMM模型拼接起来形成一个组合模型，再由该模型生成最匹配的语音。

具体而言，我们首先对语音库中的语音信号进行切分，生成上下文相关的音素，再对每个音素的长度，基频，频谱包络建立3个HMM模型。这里的上下文环境一般包括前后两个音素的标识，词内位置，句内位置，词性，韵律信息等。长度模型是一个单一状态，单个高斯的HMM,基频和频谱包络是多状态HMM。基频的状态概率包括两个空间，一个是离散空间用来描述清辅音的零基频，一个是连续空间，用来描述浊辅音和元音的非零基频。频谱包络HMM的状态概率模型是一个GMM,用来描述频谱包络在各个状态的分布规律。

在实际合成时，首先需要基于长度模型选择每个音素的发音长度，然后基于基频模型和频谱包络模型生成每个音素的基频和频谱包络，最后将基频和频谱包络送入声码器合成语音。在生成基频和频谱包络时，准则是使得生成基频和频谱包络在对应HMM中的输出概率最大化。如果不考虑语音帧之间的相关性，则每个音素的每个状态在不同时刻的输出是不变的。显然这一输出会在状态改变时产生跳跃，影响合成的质量。为解决这一问题，研究者提出了基于动态特征建模的方法。基于这一模型，每个状态的输出需要考虑前后帧的相关性，这一约束使得生成的基频和频谱包络更加连续。

如下图所示给出了基于HMM模型生成的频谱包络中的某一维的过程，其中蓝色折线是不考虑前后帧相关性的最大概率输出；当考虑前后帧约束时，生成输出变得连续，如红色曲线所示。每个圆圈代表一个HMM状态，箭头标出了每个状态生成的语音片段。下方两条水平线间的竖线表示每一帧的生成值。实际生成时，首先估计出每个音素中每个状态要生成的帧数，再对每个状态进行生成（蓝色线）。考虑到先后帧的相关性时，将生成更加平滑的语音，对应图中的红色曲线。


<div align=center>
    <img src="zh-cn/img/ch1/p10_HMM.png"  width=50%/> 
</div>
<p align=center>基于HMM模型生成某一维频谱包络的过程(基于HTS slide)</p>

下图给出了基于HMM的语音合成系统的总体框架。在训练阶段，对数据库中的语音信号进行分析，得到基频（对应声门）和频谱包络（对应声道响应），利用数据库中的标注信息，对每个上下文相关音素建立时长，基频和频谱包络的HMM模型。在合成阶段，首先对输入文本进行分析（一般称为前端处理），将文本转换成标注文件，基于该标注选择相应的音素，并将这些音素对应的基频HMM和频谱包络HMM分别串联起来，基于该串联HMM合成句子的基频和频谱包络序列，最后送入声码器得到合成语音。

<div align=center>
    <img src="zh-cn/img/ch1/p11_HMM_TTS.png"  width=50%/> 
</div>
<p align=center>基于HMM的语音合成系统的总体框架(基于HTS slide)</p>


### 5.神经模型合成

在讨论[语音识别](https://dataxujing.github.io/ASR-paper/#/)时我们已经知道，HMM模型不能描述复杂的发音现象。当数据量增加时，基于HMM的语音合成系统的性能遇到了瓶颈。为此，研究者提出基于深度神经网络（DNN）的语音合成算法。这一工作由香港中文大学，微软，Google于2013年独立提出。在这些模型中，研究者利用神经网络取代HMM模型来预测每一帧语音的激励和调制信号，在通过声码器合成自然语音。

下图是Google基于DNN的语音合成系统示意图。该系统的前端处理部分和HMM系统一样，不同的是利用DNN取代HMM来生成每个音素的时长，基频和频谱包络。事实上，Google系统中生成的是这些量的均值和方差，基于这些统计量，可以利用HMM系统类似的方法，通过考虑前后帧之间的县惯性来生成每一帧的实际参数，即图中的“参数生成”部分。生成的参数最后还是要通过一个声码器来合成语音。

<div align=center>
    <img src="zh-cn/img/ch1/p12_google_DNN_tts.png"  width=50%/> 
</div>
<p align=center>Google基于DNN的语音合成系统示意图</p>

2014年以后，研究者对DNN的合成方法进行了一系列的扩展，提出了基于RNN的合成系统。与DNN相比，RNN可以学习长时相关性，因此可以用来学习发音过程中前后音素之间的协同发音现象，从而得到更平滑，自然的发音。下图是微软发表的基于RNN的语音合成系统示意图。该模型首先生成每个音素的长度，然后基于一个双向RNN模型对每个语音帧直接预测基频和频谱包络参数。因此，该系统并不需要一个额外的参数生成器。

<div align=center>
    <img src="zh-cn/img/ch1/p13_微软_RNN_tts.png"  width=50%/> 
</div>
<p align=center>微软基于RNN的语音合成系统示意图</p>

### 6.基于注意力机制的合成系统

近两年来，基于神经网络的语音合成取得了长足的发展，其中基于注意力机制的语音合成系统最受瞩目。直观上，基于注意力机制的语音合成可类比人类的阅读-理解-复述的过程。首先，用一个双向RNN将要发音的文本进行编码，这类似于阅读和理解；然后，基于另一个RNN逐一生成每个语音帧，类似于对脑海中的记忆进行复述。在每一步生成时，系统基于注意力机制定位到发音的文本，并利用改文本的信息指导生成过程。

下图是Google基于该思路设计的Tacotron系统。在该系统中，待合成的句子以字符串的形式输入到一个编码器中，生成一个完整的句子编码。在合成时，基于RNN迭代合成每一帧参数，每合成一帧的时候，不仅将前一帧输出作为输入，同时基于注意力机制提取句子编码中相应的信息，使得生成的语音与输入要求一致。特别值得说明的是，这一方法的解码输出并不是基频和频谱包络，而是频谱本身。基于频谱可以直接合成原始语音，而不需要一个声码器。Griffin-Lim算法是一种常用的由频谱合成语音的算法，该方法首先由频谱估计出每一帧的相位，再将相位应用到频谱上，得到合成语音。Griffin-Lim算法简单，高效，但合成语音的质量不高。Google提出利用剁成卷积网络将频谱直接转化为语音，这一网络被称为WaveNet。

<div align=center>
    <img src="zh-cn/img/ch1/p14_tacotron-1.png"  width=70%/> 
</div>
<p align=center>基于注意力机制的Tacotron系统</p>


<div align=center>
    <img src="zh-cn/img/ch1/p15_wavenet.png"  width=70%/> 
</div>
<p align=center>WaveNet结构示意图</p>

如今，Tacotron已经成为语音合成系统的主流模型，可以合成非常自然的语音。人们对这一模型进行了一系列的扩展，例如DeepMind提出了一种并行的WaveNet算法，可以使得WaveNet的合成速度达到实时，Google将说话人信息表达成一个说话人向量，作为辅助输入，使得Tacotron系统可以合成不同人的声音。

### 7.来自微软的语音合成综述slide

<object data="zh-cn/img/ch1/TTS-Survey.pdf" type="application/pdf" width=100% height="530px">
    <embed src="http://www.africau.edu/images/default/sample.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="zh-cn/img/ch1/TTS-Survey.pdf">Download PDF</a>.</p>
    </embed>
</object>

<object data="zh-cn/img/ch1/TTS.ijcai21-642be55185047.pdf" type="application/pdf" width=100% height="530px">
    <embed src="http://www.africau.edu/images/default/sample.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="zh-cn/img/ch1/TTS.ijcai21-642be55185047.pdf">Download PDF</a>.</p>
    </embed>
</object>


### 8.小结

本章我们简要介绍了语音合成的基本方法。绝大部分合成方法基于激励-调制模型，该模型认为语音是声门激励进过声道的调制过程，因此，只要设计合适的激励和声道响应函数，即可合成需要的声音。最早的语音合成系统基于共振峰参数来设计声道的响应函数，基于此合成语音。这一方法效率很高，但是由于参数过于简单，质量不能保证。拼接法从大规模的数据库中直接提取声门和声道参数，可以实现较逼真的合成，但在拼接点有不连续的情况，并且占用资源较多。统计模型法是对共振峰合成的回归和扩展，利用了更多信息和更复杂的概率模型对声门和声道的参数进行预测。神经模型用更灵活的神经网络（DNN）代替HMM等统计模型，当训练数据量较大时可以突破统计模型在概率形式上的限制，从而合成更自然流畅的声音。近两年来，基于DNN的合成系统进一步发展，特别是以Tacotron为代表的的端到端系统，将合成的句子直接映射到频谱，已经摆脱了传统合成方法中对激励-调制模型的依赖，成为一种崭新的合成方法，本教程也主要介绍这些基于DNN的方法。


### 9.Reference

[1]. 《语音识别基本法 Kaldi实践与探索》

[2]. <https://github.com/dectalk/dectalk>

[3]. <https://github.com/tts-tutorial/ijcai2021>

[4]. <https://www.bilibili.com/video/BV1no4y1U7yU/?spm_id_from=333.999.0.0&vd_source=def8c63d9c5f9bf987870bf827bfcb3d>

[5]. <https://www.msra.cn/wp-content/uploads/2021/08/TTS-Survey.pdf>

[6]. <https://arxiv.org/pdf/2106.15561.pdf>

[7]. <http://hts.sp.nitech.ac.jp/>

[8]. <https://github.com/sp-nitech/DNN-HSMM>




